{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBSCAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkSzPwXvuE2JMHfYv/POly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scsanjay/ml_from_scratch/blob/main/10.%20Density-Based%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20(DBSCAN)/DBSCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_2rqsZBoSP"
      },
      "source": [
        "# Implementation of DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca1yILJKAD3v"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HabzWJWvnMfL"
      },
      "source": [
        "class DBSCAN():\n",
        "  \"\"\"\n",
        "  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) Implementation\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  eps : float, default=0.5\n",
        "      The radius to consider.\n",
        "\n",
        "  min_samples : int, default=5\n",
        "      Minimum number of samples for a dense region.\n",
        "  \n",
        "  metric : {'minkowski', 'manhattan', 'euclidean'}, default = minkowski\n",
        "      Distance measure to be used.\n",
        "\n",
        "  p : int, default=2\n",
        "      It is parameter for the minkowski distance (lp distance). It is used only\n",
        "      when metric=minkowski.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  labels_ : array of size n_samples\n",
        "      Cluster labels of each data point. -1 if it's a noisy point.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__ (self, eps=.5, min_samples=5, metric='minkowski', p=2):\n",
        "    ''' initialize params '''\n",
        "    self.eps = eps\n",
        "    self.min_samples = min_samples\n",
        "    self.metric = metric\n",
        "    self.p = p\n",
        "\n",
        "  def _getDistances(self, x_i, X):\n",
        "    ''' get distance b/w a point and all the points'''\n",
        "    if self.metric == 'euclidean':\n",
        "      p = 2\n",
        "    elif self.metric == 'manhattan':\n",
        "      p = 1\n",
        "    else:\n",
        "      p = self.p\n",
        "    # implement the distance calculation\n",
        "    distances = np.power(np.sum(np.power(np.abs(x_i-X), p), axis=1), 1/p)\n",
        "    return distances\n",
        "  \n",
        "  def _getCoreNeighbors(self, i, data, core_points, core_neighbors_all):\n",
        "    ''' recursively get density connected points '''\n",
        "    # actual core neighbors\n",
        "    core_points = set(core_points)\n",
        "    # neighbors of point i\n",
        "    neighbor_points = set(data[i][0])\n",
        "    # core neighbors of a point i\n",
        "    core_neighbors = core_points.intersection(neighbor_points)\n",
        "    # core neighbors that are not checked recursivly\n",
        "    core_neighbors_not_traversed = core_neighbors.difference(core_neighbors_all)\n",
        "    # density connected points\n",
        "    core_neighbors_all = core_neighbors_all.union(core_neighbors)\n",
        "    for j in core_neighbors_not_traversed:\n",
        "      core_neighbors_all.union(self._getCoreNeighbors(j, data, core_points, core_neighbors_all))\n",
        "    # return all density connected points\n",
        "    return core_neighbors_all\n",
        "\n",
        "    \n",
        "  def fit(self, X):\n",
        "    \"\"\"\n",
        "    It will find the clusters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of shape (n_samples, n_features)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : object\n",
        "    \"\"\"\n",
        "\n",
        "    data = {}\n",
        "    core_points = []\n",
        "    part_of_core_points = []\n",
        "    # iterate over each data\n",
        "    for i, x_i in enumerate(X):\n",
        "      # calculate distances between x_i and all the data\n",
        "      distances = self._getDistances(x_i, X)\n",
        "      # get all indices where distance is less than eps\n",
        "      distance_indices = np.argwhere(distances <= self.eps).flatten()\n",
        "      point_label = 'na'\n",
        "      # based on min_samples declare if a core point\n",
        "      if len(distance_indices) >= self.min_samples:\n",
        "        point_label = 'core'\n",
        "        # core points list\n",
        "        core_points.append(i)\n",
        "        # all neighbors of all core point\n",
        "        part_of_core_points.extend(distance_indices)\n",
        "      # save neighbors of a point\n",
        "      data[i] = [distance_indices, point_label]\n",
        "    # for non-core points check if border point or noise point\n",
        "    border_points = set()\n",
        "    for key, value in data.items():\n",
        "      if key not in core_points:\n",
        "        if key in part_of_core_points:\n",
        "          point_label = 'border'\n",
        "          border_points.add(key)\n",
        "        else:\n",
        "          point_label = 'noise'\n",
        "        data[key][1] = point_label\n",
        "    # core points that are added to a cluster\n",
        "    clustered_core_points = set()\n",
        "    # list of clusters \n",
        "    clusters = []\n",
        "    # iterate over each core point\n",
        "    for i in core_points:\n",
        "      # process if already not added to a cluster\n",
        "      if i not in clustered_core_points:\n",
        "        # get all density connected core points\n",
        "        core_neighbors = self._getCoreNeighbors(i, data, core_points, set())\n",
        "        # get all the border points of density connected core points\n",
        "        # Note some implementation may process border points separately based \n",
        "        # on closest core point's cluster but sklearn does this way\n",
        "        border_neighbors = set()\n",
        "        for j in core_neighbors:\n",
        "          border_neighbors.update(border_points.intersection(set(data[j][0])))\n",
        "        # update core points that are added to a cluster\n",
        "        clustered_core_points.update(core_neighbors)\n",
        "        # merge core points and the border points\n",
        "        core_neighbors.update(border_neighbors)\n",
        "        # save the merged points as a cluster\n",
        "        clusters.append(core_neighbors)\n",
        "    # generate cluster labels based on clusters\n",
        "    # noisy point will have -1\n",
        "    self.labels_ = np.full(len(X), -1)\n",
        "    for i, cluster in enumerate(clusters):\n",
        "      self.labels_[list(cluster)] = i\n",
        "    self.labels_ = list(self.labels_)\n",
        "    # return self\n",
        "    return self\n",
        "\n",
        "  def fit_predict (self, X):\n",
        "    \"\"\"\n",
        "    It will find the clusters and return labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of shape (n_samples, n_features)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels : array of shape (n_samples,)\n",
        "        Cluster labels of each data point. -1 if it's a noisy point.\n",
        "    \"\"\"\n",
        "    # fit the data and return labels\n",
        "    return self.fit(X).labels_"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gZ14e9BxUh"
      },
      "source": [
        "# Validate the implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXMSOSQPCeEh"
      },
      "source": [
        "X = np.array([[1, 2], [2, 2], [2, 3],\n",
        "              [8, 7], [8, 8], [7, 8], \n",
        "              [25, 80], \n",
        "              [-1, 0]])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMvzJii7B8hq"
      },
      "source": [
        "## Custom implementation output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhwSycIpFOpr"
      },
      "source": [
        "clustering = DBSCAN(eps=3, min_samples=3).fit(X)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zItXfbu8CiFC",
        "outputId": "37a3588e-d34c-4115-8478-4693ce576cae"
      },
      "source": [
        "clustering.labels_"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 1, 1, 1, -1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDPOOaaECiHa",
        "outputId": "ba41ae25-5982-4f60-a053-b581e5efe532"
      },
      "source": [
        "clustering.fit_predict(X)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 1, 1, 1, -1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR-gbeziCB0l"
      },
      "source": [
        "## sklearn's implementation output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-vdke5g7Bof",
        "outputId": "872e2a47-b2f5-41b5-be78-0c78a1624609"
      },
      "source": [
        "from sklearn import cluster\n",
        "clustering = cluster.DBSCAN(eps=3, min_samples=2).fit(X)\n",
        "print(clustering.labels_)\n",
        "print(clustering.fit_predict(X))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0  1  1  1 -1  0]\n",
            "[ 0  0  0  1  1  1 -1  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mkq3iupCIDU"
      },
      "source": [
        "**We are getting same outputs. That means the custom implementation is correct.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2B1C1qrCcZ_"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}