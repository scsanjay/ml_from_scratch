{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIn+6SjFKP5kT1V7R+RS2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scsanjay/ml_from_scratch/blob/main/11.%20K-means%20Clustering/KMeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aal5l3Bp_vfB"
      },
      "source": [
        "# Custom implementation of KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16PKcVhXS0Wi"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmM7nCgTYw5"
      },
      "source": [
        "class KMeans():\n",
        "  \"\"\"\n",
        "  KMeans clustering with Lloyds algorithm.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_clusters : int, default=8\n",
        "      The number of cluster to make.\n",
        "\n",
        "  init : {'k-means++', 'random'}, default='k-means++'\n",
        "      Initilisation of initial clusters.\n",
        "  \n",
        "  max_iter : int, default=300\n",
        "      Maximum number of iterations.\n",
        "\n",
        "  tol : float, default=1e-4\n",
        "       Tolerance to declare convergence with Frobenius norm\n",
        "       of the difference in the cluster centers of two consecutive iterations.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  labels_ : array of size n_samples\n",
        "      Cluster labels of each data point.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__ (self, n_clusters=8, init='k-means++', max_iter=300, tol=1e-4):\n",
        "    ''' initialize params '''\n",
        "    self.n_clusters = n_clusters\n",
        "    self.init = init\n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol\n",
        "  \n",
        "  def _getDistance(self, x_i, X):\n",
        "    return np.power(np.sum(np.power(np.abs(x_i-X), 2), axis=1), 1/2) \n",
        "\n",
        "  def _getInitialCentroids(self, X):\n",
        "    ''' Get initial centroids based on init param. '''\n",
        "    if self.init == 'k-means++':\n",
        "      # apply k-means++ for initial clusters\n",
        "      \n",
        "      # randomly select first centroid\n",
        "      centroids = list(np.random.choice(range(len(X)), size=1, replace=False))\n",
        "      \n",
        "      # calculate distance matrix\n",
        "      distance_matrix = []\n",
        "      for x_i in X:\n",
        "        distances = self._getDistance(x_i, X)\n",
        "        distance_matrix.append(distances)\n",
        "      distance_matrix = np.array(distance_matrix)\n",
        "\n",
        "      # select rest of the centroids\n",
        "      for i in range(self.n_clusters-1):\n",
        "        # distance from nearest cluster\n",
        "        cluster_distance_min = np.min(distance_matrix[centroids], axis=0)\n",
        "        # divide by sum so that probability sum to 1\n",
        "        np.multiply(cluster_distance_min, 1 / cluster_distance_min.sum(), cluster_distance_min)\n",
        "        # select next centroid with proportional sampling\n",
        "        centroids.append(np.random.choice(range(len(X)), size=1, replace=False, p=cluster_distance_min)[0])\n",
        "    else :\n",
        "\n",
        "      # select all centroids randomly\n",
        "      centroids = np.random.choice(range(len(X)), size=self.n_clusters, replace=False)\n",
        "\n",
        "    # return centroids\n",
        "    return centroids\n",
        "\n",
        "  def _getFrobeniusNorm(self, d):\n",
        "    ''' Calculate Frobenius Norm which is sqrt of sum of square of each elements of the matrix'''\n",
        "    d = d.flatten()\n",
        "    d = np.power(d, 2)\n",
        "    d = np.sum(d)\n",
        "    return d**.5\n",
        "\n",
        "  def fit(self, X):\n",
        "    \"\"\"\n",
        "    It will find the specified number of clusters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of shape (n_samples, n_features)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : object\n",
        "    \"\"\"\n",
        "\n",
        "    # get intial centroids\n",
        "    centroids_index = self._getInitialCentroids(X)\n",
        "    centroids = X[centroids_index]\n",
        "\n",
        "    # iterate till max\n",
        "    for z in range(self.max_iter):\n",
        "      # initiliaze empty clusters\n",
        "      clusters = {k:[] for k in range(self.n_clusters)}\n",
        "      clusters_index = {k:[] for k in range(self.n_clusters)}\n",
        "\n",
        "      # ASSIGN\n",
        "      # iterate for each data point\n",
        "      for i, x_i in enumerate(X):\n",
        "        # calculate distances to centroids for a point\n",
        "        centroid_distances = self._getDistance(x_i, centroids)\n",
        "        # get minimum distant centroid\n",
        "        min_index = np.argmin(centroid_distances)\n",
        "        # assign point to the nearest centroid\n",
        "        clusters[min_index].append(x_i)\n",
        "        clusters_index[min_index].append(i)\n",
        "      \n",
        "      # UPDATE\n",
        "      new_centroids = []\n",
        "      # iterate for each clusters\n",
        "      for _, cluster in clusters.items():\n",
        "        # calculate new centroid based on mean\n",
        "        new_centroids.append(np.mean(cluster, axis=0))\n",
        "      new_centroids = np.array(new_centroids)\n",
        "      \n",
        "      if np.abs(self._getFrobeniusNorm(centroids) - self._getFrobeniusNorm(new_centroids)) <= self.tol:\n",
        "        # break if difference of Frobenius Norm of old and new centroids is less than tolerance\n",
        "        break\n",
        "      else:\n",
        "        # set new centroid as centroid and repeat\n",
        "        centroids = new_centroids\n",
        "\n",
        "    # label each data point based on cluster\n",
        "    self.labels_ = np.zeros(len(X), dtype=np.int16)\n",
        "    for i, cluster in clusters_index.items():\n",
        "      self.labels_[list(cluster)] = i\n",
        "    self.labels_ = list(self.labels_)\n",
        "\n",
        "    # return self\n",
        "    return self\n",
        "\n",
        "  def fit_predict (self, X):\n",
        "    \"\"\"\n",
        "    It will find the clusters and return labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array of shape (n_samples, n_features)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels : array of shape (n_samples,)\n",
        "        Cluster labels of each data point. -1 if it's a noisy point.\n",
        "    \"\"\"\n",
        "    # fit the data and return labels\n",
        "    return self.fit(X).labels_"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN3lKjvJAhbe"
      },
      "source": [
        "# Validate the implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbcOL5HTd_8f"
      },
      "source": [
        "X = np.array([[1, 2], [2, 2], [2, 3],\n",
        "              [8, 7], [8, 8], [7, 8], \n",
        "              [-1, 0]])"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bb5oKgVApES"
      },
      "source": [
        "## Custom implementation output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKQLge0iWiZa",
        "outputId": "ad40eb29-930d-4ee4-c17a-2c8be9d308b1"
      },
      "source": [
        "kmeans = KMeans(n_clusters=2).fit(X)\n",
        "print(kmeans.labels_)\n",
        "print(kmeans.fit_predict(X))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 1, 1, 1, 0]\n",
            "[0, 0, 0, 1, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbZY0z41_IQD",
        "outputId": "f504ff3f-2dc4-45c7-ffda-eebb7905db29"
      },
      "source": [
        "kmeans = KMeans(n_clusters=2, init='random').fit(X)\n",
        "print(kmeans.fit_predict(X))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 1, 1, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otZMY-8y6XSe"
      },
      "source": [
        "## sklearn's implementation output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMFQVwpGd_uf"
      },
      "source": [
        "from sklearn import cluster"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDtQqREgviDC",
        "outputId": "8e4dfa8e-9de0-4ba8-9070-bb16d4db6739"
      },
      "source": [
        "kmeans = cluster.KMeans(n_clusters=2).fit(X)\n",
        "print(kmeans.labels_)\n",
        "print(kmeans.fit_predict(X))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 1 1 1 0]\n",
            "[0 0 0 1 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HhIIOY0A1Ec"
      },
      "source": [
        "**We are getting same outputs. That means the custom implementation is correct.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPGTAoJkBMTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}